<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>监督学习之KNN、朴素贝叶斯、决策树算法实验 | Welcom to Utopia land </title>
<link rel="shortcut icon" href="https://Utopian-1.github.io/favicon.ico?v=1608126594485">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://Utopian-1.github.io/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="监督学习之KNN、朴素贝叶斯、决策树算法实验 | Welcom to Utopia land  - Atom Feed" href="https://Utopian-1.github.io/atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



    <meta name="description" content="监督学习之KNN、朴素贝叶斯、决策树算法实验
一、实验目的
(1)掌握KNN及Naive Bayes、决策树的原理
(2)学会利用KNN与Navie Bayes解决分类问题
(3)熟练掌握决策树的生成方法与过程
二、实验工具
Python集..." />
    <meta name="keywords" content="" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://Utopian-1.github.io">
  <img class="avatar" src="https://Utopian-1.github.io/images/avatar.png?v=1608126594485" alt="">
  </a>
  <h1 class="site-title">
    Welcom to Utopia land 
  </h1>
  <p class="site-description">
    Hello
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu">
          归档
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="/post/about" class="menu">
          关于
        </a>
      
    
      
        <a href="/" class="menu">
          友链
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              监督学习之KNN、朴素贝叶斯、决策树算法实验
            </h2>
            <div class="post-info">
              <span>
                2020-12-16
              </span>
              <span>
                15 min read
              </span>
              
            </div>
            
            <div class="post-content-wrapper">
              <div class="post-content">
                <h1 id="监督学习之knn-朴素贝叶斯-决策树算法实验">监督学习之KNN、朴素贝叶斯、决策树算法实验</h1>
<h2 id="一-实验目的">一、实验目的</h2>
<p>(1)掌握KNN及Naive Bayes、决策树的原理</p>
<p>(2)学会利用KNN与Navie Bayes解决分类问题</p>
<p>(3)熟练掌握决策树的生成方法与过程</p>
<h2 id="二-实验工具">二、实验工具</h2>
<p>Python集成开发环境(IDE)</p>
<p>(1) Anaconda: https://www.continuum.io/  （推荐）</p>
<p>(2) IDLE: Python解释器默认工具</p>
<p>(3) PyCharm: https://www.jetbrains.com/pycharm/</p>
<p>(4) 实验数据集：Python的scikit-learn库中自带的鸢尾花数据集，可使用datasets.load_iris()载入。</p>
<h2 id="三-实验原理">三、实验原理</h2>
<p>(1) KNN（K-Nearest Neighbor）算法原理</p>
<p>存在一个样本数据集合，也称为训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类对应的关系。输入没有标签的数据后，将新数据中的每个特征与样本集中数据对应的特征进行比较，提取出样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k近邻算法中k的出处，通常k是不大于20的整数。最后选择k个最相似数据中出现次数最多的分类作为新数据的分类。</p>
<p>说明：KNN没有显示的训练过程，它是“懒惰学习”的代表，它在训练阶段只是把数据保存下来，训练时间开销为0，等收到测试样本后进行处理。</p>
<p>(2) 朴素贝叶斯</p>
<p>朴素贝叶斯分类器中最核心的便是贝叶斯准则，在机器学习中，朴素贝叶斯分类器是一个基于贝叶斯定理的比较简单的概率分类器，其中 naive（朴素）是指的对于模型中各个 feature（特征）有强独立性的假设，并未将feature间的相关性纳入考虑中。</p>
<p>朴素贝叶斯分类器一个比较著名的应用是用于对垃圾邮件分类，通常用文字特征来识别垃圾邮件，是文本分类中比较常用的一种方法。朴素贝叶斯分类通过选择 token（通常是邮件中的单词）来得到垃圾邮件和非垃圾邮件间的关联，再通过贝叶斯定理来计算概率从而对邮件进行分类。</p>
<p>(3) 决策树</p>
<p>决策树是一个非参数的监督式学习方法，主要用于分类和回归。算法的目标是通过推断数据特征，学习决策规则从而创建一个预测目标变量的模型。</p>
<h2 id="四-实验步骤">四、实验步骤</h2>
<p>需要描述清楚算法流程，包括加载数据、数据处理、创建模型，训练，预测，评估模型等。如果必须给出实现代码才能更好地说明问题时，也必须先有相关的文字叙述，然后才是代码，代码只是作为例证。</p>
<hr>
<h3 id="1导入数据">1.导入数据</h3>
<p>​		Iris数据集在模式识别研究领域应该是最知名的数据集。这个数据集里一共包括150行记录，包括鸢尾花的花萼长度，花萼宽度，花瓣长度，花瓣宽度的尺寸以及花的种类。其目的即通过鸢尾花4个属性的不同取值来预测花的种类。</p>
<p>下面导入数据：</p>
<pre><code class="language-python">from sklearn.datasets import load_iris

iris = load_iris()
data = iris.data
target = iris.target
</code></pre>
<p>下面使用train_test_split()将数据集分为训练集和测试集，随机选择训练集和数据集</p>
<pre><code class="language-python">data_train, data_test, result_train, result_test = train_test_split(data, result, test_size=0.25)
</code></pre>
<h3 id="2knn">2.KNN</h3>
<p>**算法原理：**KNN是一种基本分类与回归方法，其特点是没有显示的训练过程，是“懒惰学习”的代表，它在训练阶段只是把数据保存下来，训练时间开销为0，等收到测试样本后进行处理。  因此，他前期的模型训练过程相当简单。当收到测试样本时，则计算该样本到数据集中所有点的距离。从中选取K个距离最短的点，以这K个点作为该测试样本的参考，并采用投票的方式，少数服从多数，从而将测试样本进行分类。</p>
<p>​		而关于距离的计算，则有几种不同的选择：</p>
<p><strong>欧几里得距离（Euclidean Distance）</strong></p>
<p>欧氏距离是最常见的距离度量，衡量的是多维空间中各个点之间的绝对距离。公式如下：</p>
<figure data-type="image" tabindex="1"><img src="http://dl2.iteye.com/upload/attachment/0098/4314/bb71ff05-fe7f-3045-bfc7-1bfad452af9f.png" alt="img" loading="lazy"></figure>
<p>因为计算是基于各维度特征的绝对数值，所以欧氏度量需要保证各维度指标在相同的刻度级别，不同刻度级别可能会引起结果不准确甚至失效。</p>
<p><strong>明可夫斯基距离（Minkowski Distance）</strong></p>
<p>明氏距离是欧氏距离的推广，是对多个距离度量公式的概括性的表述。公式如下：</p>
<figure data-type="image" tabindex="2"><img src="http://dl2.iteye.com/upload/attachment/0098/4316/9567216c-ffd4-3d7f-a871-f8685a304cdd.png" alt="img" loading="lazy"></figure>
<p>这里的p值是一个变量，欧氏距离就是当p=2时的特例。</p>
<p><strong>曼哈顿距离（Manhattan Distance）</strong></p>
<p>曼哈顿距离来源于城市区块距离，是将多个维度上的距离进行求和后的结果，即当上面的明氏距离中p=1时得到的距离度量公式，如下：</p>
<figure data-type="image" tabindex="3"><img src="http://dl2.iteye.com/upload/attachment/0098/4318/87bb1b15-ee66-34ec-890e-f09a3f7aa1ab.png" alt="img" loading="lazy"></figure>
<p><strong>切比雪夫距离（Chebyshev Distance）</strong></p>
<p>切比雪夫距离起源于国际象棋中国王的走法，我们知道国际象棋国王每次只能往周围的8格中走一步，那么如果要从棋盘中A格（x1， y1）走到B格（x2， y2）最少需要走几步？扩展到多维空间，其实切比雪夫距离就是当p趋向于无穷大时的明氏距离：</p>
<figure data-type="image" tabindex="4"><img src="http://dl2.iteye.com/upload/attachment/0098/4320/069ae2ec-f9c9-3307-80e9-0d3fb9d0d5dd.png" alt="img" loading="lazy"></figure>
<h3 id="3朴素贝叶斯">3.朴素贝叶斯</h3>
<p>**算法原理：**朴素贝叶斯分类是贝叶斯分类器中的一种，也是最简单，最常用的分类器。朴素贝叶斯之所以称之为朴素，是因为它假设属性是相互独立的，这是一个强硬的假设，实际情况并不一定，如果属性之间存在关联，分类准确率会降低。但是这项技术对于绝大部分的复杂问题仍然非常有效。</p>
<p>​		朴素贝叶斯模型由两种类型的概率组成：</p>
<ul>
<li>
<p>每个类别的概率 P(Cj)；</p>
</li>
<li>
<p>每个属性的条件概率 P(Ai|Cj)。</p>
</li>
</ul>
<p>类别概率就是计算样本的目标分类的不同类别的概率，如“好瓜”和“坏瓜”的概率，“男性”和“女性”的概率。</p>
<p>条件概率就是已知样本的某个条件，计算其分在某类的概率。如已知瓜的“纹理”=“清晰”，其为好瓜的概率</p>
<p>​		为了训练朴素贝叶斯模型，我们需要先给出训练数据，以及这些数据对应的分类。那么类别概率和条件概率。他们都可以从给出的训练数据中计算出来。一旦计算出来，概率模型就可以使用贝叶斯原理对新来的数据进行预测。</p>
<p>根据训练数据的类别，可以分为<strong>离散数据</strong>的情况和<strong>连续数据</strong>的情况。但其所依赖的数学原理均为以下公式：</p>
<figure data-type="image" tabindex="5"><img src="%E5%AE%9E%E9%AA%8C%E4%B8%80.assets/image-20201216203344551.png" alt="image-20201216203344551" loading="lazy"></figure>
<h5 id="离散数据情况">离散数据情况</h5>
<p>如给定如下数据集：</p>
<figure data-type="image" tabindex="6"><img src="https://static001.geekbang.org/resource/image/de/5d/de0eb88143721c4503d10f0f7adc685d.png" alt="img" loading="lazy"></figure>
<p>现在已知一个新的数据，即已知身高，体重，鞋码，现在要预测其性别。</p>
<p>假设用 A1, A2, A3 代表这三个已知的属性，用 C 代表类别，则 C1,C2 分别是：男、女，在未知的情况下用 Cj 表示。</p>
<p>现在目的为在已知 A1、A2、A3的情况下，求Cj 的概率，用条件概率表示就是 P(Cj|A1A2A3)。根据上面讲的贝叶斯的公式，我们可以得出：</p>
<img src="https://static001.geekbang.org/resource/image/55/64/556ae2a160ce37ca48a456b7dc61e564.png" alt="img" style="zoom:50%;" />
<p>由于假定 Ai 之间是相互独立的，则有：</p>
<img src="实验一.assets/image-20201216204617480.png" alt="image-20201216204617480" style="zoom:50%;" />
<p>这样变可以求出现有样本属于某一类别的概率，选择较大的一方即可。</p>
<p><strong>连续数据情况</strong></p>
<p>​		当训练样本足够多的时候，也可以对样本进行分区间处理，即把连续数据离散化。采用上面介绍的方法进行计算。但当数据样本较少时，则可假设样本的各个属性服从正态分布，利用正态分布来求得，在代入公式进行计算。</p>
<p><strong>不足及改进：</strong></p>
<p>​		若某个属性值在训练集中没有与某个类同时出现过，则直接基于公式进行概率估计，将出现问题。此时分子出现了0，因此，无论该样本的其他属性是什么，哪怕在其他属性上明显偏向属性A，分类的结果都将属于属性B。这样显然不合理。</p>
<p>​		为了避免其他属性携带的信息被训练集中未出现的属性值“抹去”，在估计概率值时通常要进行“平滑”(smoothing),常用“拉普拉斯修正”(Laplacian correction)。</p>
<h3 id="4决策树">4.决策树</h3>
<p>决策树是一个包含根节点、若干内部节点和若干叶子节点的树形结构。决策树的根节点包含样本全集，内部节点对应特征属性，叶子节点表示决策的结果。</p>
<p>使用决策树进行判断时，从根节点开始，测试待分类数据的特征属性值，应该走哪条分支，循环这样判断，直到叶子节点为止。最终到达的这个叶子节点，就是决策树的最终决策结果。</p>
<p>决策树模型的学习过程一般有三个阶段：</p>
<ul>
<li><strong>特征选择</strong>：选择哪些属性作为树的节点。</li>
<li><strong>生成决策树</strong>：生成树形结构。</li>
<li><strong>决策树剪枝</strong>：是决策树的一种优化手段，比如剪去一些不必要的属性节点。一般有“预剪枝”和“后剪枝”两种。
<ul>
<li>剪枝的<strong>目的</strong>是防止过拟合现象，提高泛化能力。</li>
<li><strong>预剪枝</strong>是在决策树的生成过程中就进行剪枝，缺点是有可能造成欠拟合。</li>
<li><strong>后剪枝</strong>是在决策树生成之后再进行剪枝，缺点是计算量较大。</li>
</ul>
</li>
</ul>
<p>决策树模型建立的最关键的是选择最优化分属性，期望决策树的分支节点所包含的样本尽可能属于同一类别。划分方法的不同对应了3中最重要的决策树算法。</p>
<h4 id="1信息增益id3算法">1.信息增益（ID3算法）</h4>
<p>ID3算法尝试对所有属性进行划分，分别计算划分前后数据的熵的变化，选择信息熵减小的最多的属性进行下一步的划分。信息熵的公式如下：</p>
<img src="实验一.assets/image-20201216211810851.png" alt="image-20201216211810851" style="zoom: 67%;" />
<h4 id="2增益率c45算法">2.增益率（C4.5算法）</h4>
<p>因为 ID3 在计算的时候，倾向于选择取值多的属性。为了避免这个问题，C4.5 采用信息增益率的方式来选择属性。信息增益率 = 信息增益 / 属性熵，具体的计算公式这里省略。</p>
<p>当属性有很多值的时候，相当于被划分成了许多份，虽然信息增益变大了，但是对于 C4.5 来说，属性熵也会变大，所以整体的信息增益率并不大。所以C4.5算法是ID3算法的一个改进版本。</p>
<h4 id="3基尼指数cart决策树算法">3.基尼指数（CART决策树算法）</h4>
<p>CART 算法，英文全称叫做 Classification And Regression Tree，中文叫做分类回归树。ID3 和 C4.5 算法可以生成二叉树或多叉树，而 CART 只支持二叉树。同时 CART 决策树比较特殊，既可以作分类树，又可以作回归树。</p>
<p>CART 分类树与 C4.5 算法类似，只是属性选择的指标采用的是基尼系数。基尼系数本身反应了样本的不确定度。当基尼系数越小的时候，说明样本之间的差异性小，不确定程度低。分类的过程本身是一个不确定度降低的过程，即纯度的提升过程。所以 CART 算法在构造分类树的时候，会选择基尼系数最小的属性作为属性的划分。</p>
<hr>
<p>决策树构造出来之后可能还需要对决策树进行剪枝。来防止过拟合现象。一般来说，剪枝可以分为“预剪枝”（Pre-Pruning）和“后剪枝”（Post-Pruning）。</p>
<p>预剪枝是在决策树构造时就进行剪枝。方法是在构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为叶节点，不对其进行划分。</p>
<p>后剪枝就是在生成决策树之后再进行剪枝，通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。方法是：用这个节点子树的叶子节点来替代该节点，类标记为这个节点子树中最频繁的那个类。</p>
<h2 id="五-实验数据及处理结果">五、实验数据及处理结果</h2>
<h3 id="knn">KNN</h3>
<p>由于KNN算法较为简单，在此使用Python编写实现。</p>
<pre><code>from sklearn.datasets import load_iris
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


def distance(pointA, pointB):  # 计算A，B两点之间的距离
    num = len(pointA)
    dis = 0
    for i in range(num):
        dis = dis + (pointA[i] - pointB[i]) ** 2
    return dis


def find_min(list, num):  # 在list中找出最小的num个数的下标
    min_index = []
    List = list[:]
    for i in range(num):
        curr_min_index = List.index(min(List))
        min_index.append(curr_min_index)
        List[curr_min_index] = float('inf')
    return min_index


def find_most(List):  # 在一个列表里找到出现次数最多的元素
    Count = [List.count(0), List.count(1), List.count(2)]
    return Count.index(max(Count))


iris = load_iris()
data = iris.data
target = iris.target
data_train, data_test, result_train, result_test = train_test_split(data, target, test_size=0.2)

K = 10  # 观测距离最近的10个数
predict = []
for data in data_test:
    dis_list = []
    for i in range(len(data_train)):
        dis_list.append(distance(data, data_train[i]))
    min_index = find_min(dis_list, K)
    belong_to_class = []
    for ele in min_index:
        belong_to_class.append(result_train[ele])
    predict.append(find_most(belong_to_class))
print(accuracy_score(result_test, predict))
</code></pre>
<p>调用load_iris()里的150条数据，将其分为120条训练数据及30条测试数据用于计算其准确性。</p>
<p>由结果可知，测试结果较为合理，达到了90%以上的预测准确率。</p>
<img src="实验一.assets/image-20201216213104940.png" alt="image-20201216213104940" style="zoom: 67%;" />
<h4 id="朴素贝叶斯">朴素贝叶斯</h4>
<p>朴素贝叶斯的算法手动编写较为困难，在此使用sklearn.naive_bayes()第三方库进行对该算法的验证。由于sklearn.naive_bayes()内含有多种贝叶斯分类器，各种各样的的朴素贝叶斯分类器的差异大部分来自于处理分布时的所做的假设不同，在此使用高斯朴素贝叶斯分类器进行验证。</p>
<pre><code>from sklearn.datasets import load_iris
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

def get_data():
    iris = load_iris()
    data = iris.data
    result = iris.target
    return data,result

data,result = get_data()
data_train,data_test,result_train,result_test = train_test_split(data,result, test_size=0.3)

Gauss = GaussianNB()
Gauss = Gauss.fit(data_train,result_train)
pred = Gauss.predict(data_test)

print(accuracy_score(result_test,pred))
</code></pre>
<p>预测精度也较为合理：</p>
<img src="实验一.assets/image-20201216214216197.png" alt="image-20201216214216197" style="zoom:67%;" />
<h4 id="决策树">决策树</h4>
<pre><code>from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


def get_data():
    iris = load_iris()
    data = iris.data
    result = iris.target
    return data, result


data, result = get_data()
data_train, data_test, result_train, result_test = train_test_split(data, result, test_size=0.3)
Tree = DecisionTreeClassifier()
Tree.fit(data_train, result_train)
pred = Tree.predict(data_test)

print(accuracy_score(result_test, pred))
</code></pre>
<p>验证精确度：</p>
<img src="实验一.assets/image-20201216214552634.png" alt="image-20201216214552634" style="zoom:67%;" />
<h2 id="六-实验结论">六、实验结论</h2>
<h5 id="1knn">1.KNN</h5>
<p><strong>优点</strong>	简单，易于理解和实现，无需估计参数，也不需要提前对训练集进行训练</p>
<p>**缺点	**计算量较大，对一个未知个体分类时需要计算它到全体已知样本的距离</p>
<h5 id="2朴素贝叶斯">2.朴素贝叶斯</h5>
<p>**优点	**稳定的分类效率。对缺失数据不太敏感，算法也比较简单</p>
<p>**缺点	**某些时候会由于假设的先验模型的原因导致预测效果不佳</p>
<h5 id="3决策树">3.决策树</h5>
<p><strong>优点</strong>	计算复杂度不高，算法过程符合人的直觉，易于理解，对离散属性的处理较好</p>
<p><strong>缺点</strong>	对连续性属性比较难预测。对有时间顺序的数据，需要很多预处理的工作。当类别太多时，错误可能就会增加的比较快。</p>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li><a href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8Bknn-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF-%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E5%AE%9E%E9%AA%8C">监督学习之KNN、朴素贝叶斯、决策树算法实验</a>
<ul>
<li><a href="#%E4%B8%80-%E5%AE%9E%E9%AA%8C%E7%9B%AE%E7%9A%84">一、实验目的</a></li>
<li><a href="#%E4%BA%8C-%E5%AE%9E%E9%AA%8C%E5%B7%A5%E5%85%B7">二、实验工具</a></li>
<li><a href="#%E4%B8%89-%E5%AE%9E%E9%AA%8C%E5%8E%9F%E7%90%86">三、实验原理</a></li>
<li><a href="#%E5%9B%9B-%E5%AE%9E%E9%AA%8C%E6%AD%A5%E9%AA%A4">四、实验步骤</a>
<ul>
<li><a href="#1%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE">1.导入数据</a></li>
<li><a href="#2knn">2.KNN</a></li>
<li><a href="#3%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF">3.朴素贝叶斯</a><br>
*
<ul>
<li><a href="#%E7%A6%BB%E6%95%A3%E6%95%B0%E6%8D%AE%E6%83%85%E5%86%B5">离散数据情况</a></li>
</ul>
</li>
<li><a href="#4%E5%86%B3%E7%AD%96%E6%A0%91">4.决策树</a>
<ul>
<li><a href="#1%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8Aid3%E7%AE%97%E6%B3%95">1.信息增益（ID3算法）</a></li>
<li><a href="#2%E5%A2%9E%E7%9B%8A%E7%8E%87c45%E7%AE%97%E6%B3%95">2.增益率（C4.5算法）</a></li>
<li><a href="#3%E5%9F%BA%E5%B0%BC%E6%8C%87%E6%95%B0cart%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95">3.基尼指数（CART决策树算法）</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E4%BA%94-%E5%AE%9E%E9%AA%8C%E6%95%B0%E6%8D%AE%E5%8F%8A%E5%A4%84%E7%90%86%E7%BB%93%E6%9E%9C">五、实验数据及处理结果</a>
<ul>
<li><a href="#knn">KNN</a>
<ul>
<li><a href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF">朴素贝叶斯</a></li>
<li><a href="#%E5%86%B3%E7%AD%96%E6%A0%91">决策树</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E5%85%AD-%E5%AE%9E%E9%AA%8C%E7%BB%93%E8%AE%BA">六、实验结论</a><br>
*<br>
*<br>
* <a href="#1knn">1.KNN</a><br>
* <a href="#2%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF">2.朴素贝叶斯</a><br>
* <a href="#3%E5%86%B3%E7%AD%96%E6%A0%91">3.决策树</a></li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://Utopian-1.github.io/post/dui-cheng-mi-ma-fen-xi-wei-ji-ni-ya-mi-ma-de-po-jie/">
              <h3 class="post-title">
                对称密码分析-维吉尼亚密码的破解
              </h3>
            </a>
          </div>
        

        

        <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
  <a class="rss" href="https://Utopian-1.github.io/atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
